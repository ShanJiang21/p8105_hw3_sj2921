---
title: "p8105_hw3_sj2921"
author: "Shan Jiang"
date: "10/5/2018"
output: github_document
---
## Problem 1
```{r}
# Set the graph properties(theme, color and size) for ggplot
library(ggplot2)
library(readxl)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_bw() + theme(legend.position = "bottom"))
```
* Import the data
```{r echo=FALSE,results='hide'}
library(tidyverse)
library(p8105.datasets)
# Import the BFRSS data, exclude variables and format the data by using appropriate names.
brfss_df <- 
    brfss_smart2010 %>% 
    janitor::clean_names() %>% 
    filter(response == "Excellent"  | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") 

```
* structure data: Response indicating the proportion of subjects with each response
```{r}
# Convert response to build a factor; Specify the levels of response_vector
response <- 
  factor(brfss_df$response, 
  levels <- (c("Excellent","Very Good", "Good", "Fair", "Poor")))


```
##### Questions
* 1.In 2002,  CT, MT, NH, NM, OR, TN, UT were observed at 7 locations.
```{r}
 brfss_df %>% 
  group_by(locationabbr) %>% 
  distinct(locationdesc) %>% 
  summarise(n = n()) %>% 
  filter(n == 7)
```
* 2.Make a spaghetti plot showing the number of observations in each state from 2002 to 2010
```{r}
location <- brfss_df %>% 
  group_by(locationabbr, year) %>% 
  summarise(n = n()) 

ggplot(location, aes(x = year, y = n)) +
  geom_line(alpha = .3, aes(group = locationabbr)) +
  geom_point(alpha = .3) +
  labs(
    title = "Number of observations in each state(n) from 2002 to 2010",
    x = "Year",
    y = "Number of observations in each state(n)",
    caption = "Data from the BRFSS package"
  ) + scale_color_hue(name = "locationabbr", 
                  h = c(200, 300))
```
* 3. Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.
```{r}
brfss_df %>% 
  filter(locationabbr == "NY" & response == "Excellent") %>%
  filter(year == "2002" | year == "2006" | year == "2010") %>% 
  group_by(year) %>% 
  summarise(mean_ex = mean(data_value),
            sd_ex = sd(data_value)) %>% 
knitr::kable(digits = 1)
```
* 4.1 For each year and state, compute the average proportion in each response category (taking the average across locations in a state). 
```{r}
panel <- brfss_df %>%
  group_by(locationabbr, year) %>% 
  summarise(n = n(),
            avg_pro = mean(data_value))
```
* 4.2 Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.
```{r}
ggplot(panel, aes(x = year, y = avg_pro )) +
  geom_point()


```

# problem 2

##### The dataset cleaning and summarizing 

**Instacart Dataset description**

1. There are **1384617 observations** and **15** variables in the dataset. The dataset is a long dataset using the order_id and user_id as main keys. 
```{r}
# 21 departments: summary
instacart %>% 
  janitor::clean_names() %>% 
  group_by(department) %>% 
  summarize(n = n()) %>% 
  mutate(dep_ranking = min_rank(desc(n))) %>% 
  filter(dep_ranking > 20 | dep_ranking < 4)
```

2. We know that there are in total 21 departments. According to the ranking for no.of orders in each department, the 3 top-ranked department are **produce, dairy eggs and snacks**. The produce department has received 409087 orders which are the double of the second-ranked dairy eggs. The least popular department is **bulk**, which only has 1359 orders.

```{r}
# The top-3 buyers for placing the most orders
instacart %>% 
  janitor::clean_names() %>% 
  group_by(user_id) %>% 
  summarize(n = n()) %>% 
  mutate(user_ranking = min_rank(desc(n))) %>% 
  filter(user_ranking < 3)
```
3. The one who has the highest purchasing power among users are the two **user_id:149753, user_id:197541**.Both of them have 80 orders and they have maintained a high reorder rate. 
```{r}
# The most popular products
instacart %>% 
  janitor::clean_names() %>% 
  group_by(product_name) %>% 
  summarize(n = n()) %>% 
  mutate(pro_ranking = min_rank(desc(n))) %>% 
  filter(pro_ranking < 3)
```

4. The most popular product are **Bananas** which appeared in 18726 orders. Bag of Organic Bananas, following as the second, they appeared in 15480 orders, so the supply of bananas should be addressed.

Questions
-
1. How many aisles are there, and which aisles are the most items ordered from?
```{r}

```
2.


3.


# problem 3
* NY NOAA data
1.load the data
```{r}
# Import the ny_noaa data, exclude variables and format the data by using appropriate names.
    ny_noaa %>% 
    janitor::clean_names() 

```
2.a short description of the dataset
The ny_noaa data set contains 7 variables and 2595176 observation value.

- some key variables include: 
* prcp: The daily precipitation volume in the unique location(id as key). 
* snow: The daily snowfall volume in the unique location(id as key)
* snwd: South to North Water diversion in the specific date.
```{r results='hide'}
sum(is.na(ny_noaa$prcp))
sum(is.na(ny_noaa$snow))
sum(is.na(ny_noaa$snwd))
```
3. missing data percentage is an issue for snow and snwd as the percentage of missing values accounts for more than 22% for snwd and over 14% for snow.
* prcp:  `r sum(is.na(ny_noaa$prcp))/2595176`  
* snow:  `r sum(is.na(ny_noaa$snow))/2595176`  
* snwd:  `r sum(is.na(ny_noaa$snwd))/2595176`

4. Create separate variables for year, month, and day.
```{r}
# Mutate separate variables for year, month, and day.
ny_noaa <-
  ny_noaa %>% 
   mutate(year = as.numeric(format(date, format = "%Y")),
                 month = as.numeric(format(date, format = "%m")),
                 day = as.numeric(format(date, format = "%d"))) 
# Convert the month into the month name
ny_noaa$month <- as.character(month.name[ny_noaa$month])
  
```
5. For snowfall, what are the most commonly observed values? Why?
```{r}
ny_noaa %>% 
  group_by(snow) %>%
  summarize(n = n()) %>% 
  filter(min_rank(desc(n)) < 2)
```
The most commonly observed value is 0 as the snowfall is observed by days.Usually it just snow at specific low temp or winter season, so the most commonly value appeared here is 0.

6. two-panel plot showing the average temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?
```{r}
# Convert the temp from character to interger vector.
ny_noaa$tmin <- as.numeric(ny_noaa$tmin)
  ny_noaa$tmax <- as.numeric(ny_noaa$tmax)
  
# Average temperature in January and in July in each station across years.
temp_mean <- ny_noaa %>%
  filter(month == "January" | month == "July" ) %>% 
  group_by(id, year, month) %>% 
  summarise(
    mean_tem = mean(tmax + tmin , na.rm = T)
  )
# detect the na value in the dataset
sum(is.na(max_mean))

# Graphing the temp 
ggplot(temp_mean, aes(x = year , y =  mean_tem, color = id)) +
  geom_point() 

```
* The outliers:

* 
7. Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
```{r}



```

