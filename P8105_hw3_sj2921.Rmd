---
title: "p8105_hw3_sj2921"
author: "Shan Jiang"
date: "10/5/2018"
output: github_document
---
## Problem 1
```{r}
# Set the graph properties(theme, color and size) for ggplot
library(ggplot2)
library(readxl)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_bw() + theme(legend.position = "bottom"))
```
* Import the data
```{r echo=FALSE,results='hide'}
library(tidyverse)
library(p8105.datasets)
# Import the BFRSS data, exclude variables and format the data by using appropriate names.
brfss_df <- 
    brfss_smart2010 %>% 
    janitor::clean_names() %>% 
# Focus on overall health topic
    filter(topic == "Overall Health") %>%
    separate(locationdesc, into = c('state', 'location'), sep = ' - ') %>%
    filter(response == "Excellent"  | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") %>% 
    select(-locationabbr, -class, -topic,  -question, -c(confidence_limit_low:geo_location))

```

* structure data: Response indicating the proportion of subjects with each response
```{r}
# Convert response to build a factor; Specify the levels of response_vector
response <- 
  factor(brfss_df$response, 
  levels <- (c("Excellent","Very Good", "Good", "Fair", "Poor")))

```
##### Questions
* 1.In 2002,  CT, FL, NC were observed at 7 locations.
```{r}
 brfss_df %>% 
  filter(year == "2002") %>% 
  group_by(state) %>% 
  distinct(location) %>% 
  summarise(n = n()) %>% 
  filter(n == 7)
```
* 2.Make a spaghetti plot showing the number of observations in each state from 2002 to 2010
```{r}
location <- brfss_df %>% 
  group_by(state, year) %>% 
  summarise(n = n()) 

ggplot(location, aes(x = year, y = n, color = state)) +
  geom_line(alpha = .3, aes(group = state)) +
  geom_point(alpha = .3) +
  labs(
    title = "Number of observations in each state(n) from 2002 to 2010",
    x = "Year",
    y = "Number of observations in each state(n)",
    caption = "Data from the BRFSS package"
  )  + 
  viridis::scale_color_viridis(
    name = "State",
    discrete = T
  ) 
```
* 3. Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.
```{r}
brfss_df %>% 
  filter(state == "NY" & response == "Excellent") %>%
  filter(year == "2002" | year == "2006" | year == "2010") %>% 
  group_by(state, year) %>% 
  summarise(mean_ex = (mean(data_value, na.rm = T)),
            sd_ex = sd(data_value, na.rm = T)) %>% 
 

knitr::kable(digits = 2)
```
* Comments: From the graph above, we can see some outliers in the time-trend graph, as the Florida has two outliers in two years, so it shows a big variability across years.

* 4.1 For each year and state, compute the average proportion in each response category (taking the average across locations in a state). 
```{r}
panel <- brfss_df %>%
  group_by(state, year, response) %>% 
  summarise(n = n(),
            avg_pro = mean(data_value, na.rm = T)) 
```
* 4.2 Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.
```{r}
ggplot(panel, aes(x = year, y = avg_pro, color = state)) + geom_line(alpha = 0.4) +
  labs(
    x = "Year",
    Y = "state-level averages",
    title = "Distribution of state-level response averages over time",
    caption = "Data from the BRFSS package"
  ) +
  facet_grid(~response) +
  viridis::scale_color_viridis(
    name = "state", 
    discrete = TRUE,
    option = "inferno"
  ) 

```
* Comments: 
(1). The five panel plot shows that overall health condition among states through the years from 2002 to 2010. 
(2). The response of 'good' and 'very good' has a high percentage for all states around the country, implying that the health status for the general public of all states tend to concerntrates on the good or above as fair and poor are at a rather low percentage of the whole dataset. 
(3). The between group difference among the State-level groups are not significant as the line shown in the graph tends to overlap with each other. 
(4). For the time series data analysis, the trend of health status for state-level groups are stable as there is no evident flutuations over the years for these 5 panels.

# problem 2

##### The dataset cleaning and summarizing 

**Instacart Dataset description**

1. There are **1384617 observations** and **15** variables in the dataset. The dataset is a long dataset using the `order_id` and `user_id` as keys to identify distinct entries. 

* Key variables in the dataset
1. Both `aisle_id` and `aisle` are variable for identifying the location of products, which denotes the aisle for different products placement.

```{r results='hide'}
# 21 departments: summary
depart <- instacart %>%  
  janitor::clean_names() %>% 
  distinct(department)  
```
2.`department_id` is a integer variable, for implying the department the products belong to, there are 21 distinct departments in total.
```{r}
# ranking the topr 3 and the laat departments: summary
instacart %>%  
  janitor::clean_names() %>% 
  group_by(department) %>% 
  summarize(n = n()) %>% 
  mutate(dep_ranking = min_rank(desc(n))) %>% 
  filter(dep_ranking > 20 | dep_ranking < 4)
```
* **Department Popularity Ranking**: According to the ranking for no.of orders in each department, the 3 top-ranked department are **produce, dairy eggs and snacks**. The produce department has received 409087 orders which are the double of the second-ranked dairy eggs. The least popular department is **bulk**, which only received 1359 orders.
```{r}
user_unique <- instacart %>% 
  distinct(user_id) 
  
```
3. `user_id` is used for marking each customers in the dataset, there are `r sum(user_unique)` customers signed up in the symstem.

4. `order_id`, `reordered` and `add_to_cart_order` are used as integer to attach info for orders from users.There is a mean of `r mean(instacart$add_to_cart_order, na.rm =T)` add to cart order in the dataframe. 

5. The one who has the highest purchasing power among users are the two **user_id:149753, user_id:197541**.Both of them have 80 orders and they have maintained a high reorder rate. 
The top-3 buyers for placing the most orders are presented as: 
```{r}
# The top-3 buyers for placing the most orders
instacart %>% 
  janitor::clean_names() %>% 
  group_by(user_id) %>% 
  summarize(n = n()) %>% 
  mutate(user_ranking = min_rank(desc(n))) %>% 
  filter(user_ranking < 3)
```

6. Products are marked by the variable of and `product_id` 
```{r}
# The most popular products
instacart %>% 
  janitor::clean_names() %>% 
  group_by(product_name) %>% 
  summarize(n = n()) %>% 
  mutate(pro_ranking = min_rank(desc(n))) %>% 
  filter(pro_ranking < 3)
```
* The most popular product are **Bananas** which appeared in 18726 orders. Bag of Organic Bananas, following as the second, they appeared in 15480 orders, so the supply of bananas should be addressed.

7. `order_dow` means the day in the week when the order was established.

#### Questions
-
1. How many aisles are there, and which aisles are the most items ordered from?
```{r}
aisle <- instacart %>% 
  distinct(aisle_id) 
## Rank and find the aisle most items ordered from
instacart %>% 
  group_by(aisle) %>% 
  summarize(n = n()) %>% 
  mutate(aisle_ranking = min_rank(desc(n))) %>% 
  filter(aisle_ranking < 2) 

## Find out the aisle_id corresponding to the fresh vegetables;
instacart %>% 
  group_by(aisle_id) %>% 
  summarize(n = n()) %>% 
  mutate(aisle_ranking = min_rank(desc(n))) %>% 
  filter(aisle_ranking < 2) 
```
* There are in total **`r sum(aisle)`** aisles in the dataset. 
* **Aisle fresh vegetables** is the aisle most items ordered from.

2. Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.

  
```{r}
# The top 5 aisle "Best seller"
aisle_graph <- instacart %>%
  group_by(aisle) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n)) %>% 
  top_n(5) 

# Order the aisle sensibly for their department category
aisle_df <- instacart %>%
    group_by(aisle, department) %>% 
    summarize(n = n()) %>% 
    arrange(desc(n)) %>% 
    filter(n >= 23000) %>% 
    ggplot(aes(x = aisle, y = n, color = department)) +
           geom_bar(position = "fill",stat = "identity") +
   viridis::scale_color_viridis(
    name = "aisle id",
    discrete = T
   ) +
  coord_flip()
 

# plot the aisle point graph 



# plot the aisle product number 
ggplot(aisle_graph, aes(aisle, n, colour = factor(n) )) +
  geom_col( alpha = 0.5) +
  labs(
    title = "Items ordered in each aisle",
    x = "aisle",
    y = "No. of products",
    caption = "Data from the instacart package"
  ) + scale_color_hue(name = "Aisle", 
                  h = c(50, 900)) + scale_x_discrete(labels = function(x) str_wrap(x, width = 90)) +
  coord_flip()


```




3. most popular item in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
```{r}
library(knitr)
# Filter the products in aisles “baking ingredients”
rank_3 <- instacart %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarise(n = n()) %>% 
  mutate(product_ranking = min_rank(desc(n))) %>% 
  filter(product_ranking < 2) 

# Print the table with knitr
knitr::kable(rank_3, caption = "most popular item in each of the aisles")
```

4. Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).
```{r}
# mean hour of the day 
instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_time = round(mean(order_hour_of_day), digits = 2) ) %>% 
  spread(key = "order_dow", value = "mean_time") %>% 
  knitr::kable(caption = "mean hour of the day 2 items are ordered ")
```


# problem 3
* NY NOAA data
1.load the data
```{r}
# Import the ny_noaa data, exclude variables and format the data by using appropriate names.
ny_noaa <- ny_noaa %>% 
    janitor::clean_names() %>% 
     mutate(prcp = as.numeric(prcp) / 10,
         tmax = as.numeric(tmax) / 10,
         tmin = as.numeric(tmin) / 10)

```
2.a short description of the dataset
The ny_noaa data set contains 7 variables and 2595176 observation value.

- some key variables include: 
* prcp: The daily precipitation volume in the unique location(id as key). 
* snow: The daily snowfall volume in the unique location(id as key)
* snwd: South to North Water diversion in the specific date.
```{r results='hide'}
sum(is.na(ny_noaa$prcp))
sum(is.na(ny_noaa$snow))
sum(is.na(ny_noaa$snwd))
```
3. missing data percentage is an issue for snow and snwd as the percentage of missing values accounts for more than 22% for snwd and over 14% for snow.
* prcp:  `r sum(is.na(ny_noaa$prcp))/2595176`  
* snow:  `r sum(is.na(ny_noaa$snow))/2595176`  
* snwd:  `r sum(is.na(ny_noaa$snwd))/2595176`

4. Create separate variables for year, month, and day.
```{r}
# Mutate separate variables for year, month, and day.
ny_noaa <-
  ny_noaa %>% 
   mutate(year = as.numeric(format(date, format = "%Y")),
                 month = as.numeric(format(date, format = "%m")),
                 day = as.numeric(format(date, format = "%d"))) 
# Convert the month into the month name
ny_noaa$month <- as.character(month.name[ny_noaa$month])
  
```
5. For snowfall, what are the most commonly observed values? Why?
```{r}
ny_noaa %>% 
  group_by(snow) %>%
  summarize(n = n()) %>% 
  filter(min_rank(desc(n)) < 2)
```
The most commonly observed value is 0 as the snowfall is observed by days.Usually it just snow at specific low temp or winter season, so the most commonly value appeared here is 0.

6. two-panel plot showing the average temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?
```{r}
# Convert the temp from character to interger vector.
ny_noaa$tmin <- as.numeric(ny_noaa$tmin)
  ny_noaa$tmax <- as.numeric(ny_noaa$tmax)
  
# Average temperature in January and in July in each station across years.
temp_mean <- ny_noaa %>%
  filter(month == "January" | month == "July" ) %>% 
  group_by(id, year, month) %>% 
  summarise(
    mean_tem = mean(tmax + tmin , na.rm = T)
  )
# detect the na value in the dataset
sum(is.na(max_mean))

# Graphing the temp 
ggplot(temp_mean, aes(x = year , y =  mean_tem, color = id)) +
  geom_violin () 

```
* The outliers:

* 
7.  (i) Make a two-panel plot showing tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); 
```{r}
ny_noaa %>% 
  ggplot(aes(x = tmax, y = tmin)) + 
    geom_line(na.rm = TRUE) +
    ggtitle(title = "hexagonal heatmap of maximum temperature vs. minimum temperature" ) +
    theme(text = element_text(size = 18)) + 
  facet_grid()


```

8.  (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
```{r}
ny_noaa %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow, fill = year)) + 
  geom_boxplot()
```

